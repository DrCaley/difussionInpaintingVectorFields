{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9504399c",
   "metadata": {},
   "source": [
    "# Train Attention UNet (MyUNet_Attn) on Colab\n",
    "\n",
    "Trains the `repaint_gaussian_attn` experiment — Guided-Diffusion-style UNet with\n",
    "self-attention for ocean velocity inpainting via RePaint.\n",
    "\n",
    "**Architecture:** MyUNet_Attn (23.1M params)\n",
    "- ResBlocks with AdaGN time conditioning + residual skip connections\n",
    "- Multi-head self-attention at 16×32, 8×16, and 4×8 resolutions\n",
    "- Channel schedule: 64 → 128 → 256 → 256, bottleneck 4×8\n",
    "\n",
    "**Training improvements (Feb 2026):**\n",
    "- **Cosine LR schedule** with linear warmup (breaks through Adam plateau)\n",
    "- **AdamW** with weight decay (better generalization)\n",
    "- **EMA** of model weights (smoother inference, standard in DDPM)\n",
    "- **Velocity-aware augmentation** (H/V flips preserving ∇·v=0)\n",
    "- **Full test-set evaluation** (all 1965 samples, not just 320)\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Push latest code to GitHub (including `unet_xl_attn.py`)\n",
    "2. Upload `stjohn_hourly_5m_velocity_ramhead_v2.mat` (942 MB) to Google Drive\n",
    "   - Put it in: `My Drive/Ocean Inpainting/`\n",
    "3. Upload `boundaries.yaml` to the same Drive folder\n",
    "\n",
    "**Runtime:** Select GPU runtime (Runtime → Change runtime type → A100 or V100)\n",
    "\n",
    "**Estimated time:** ~6-10 hours for 1000 epochs on A100 (full eval takes longer per epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab98861",
   "metadata": {},
   "source": [
    "## 1. Setup: Mount Drive, Clone Repo, Install Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509912d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for data + saving checkpoints)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8883553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo\n",
    "import os\n",
    "REPO_URL = 'https://github.com/DrCaley/difussionInpaintingVectorFields.git'\n",
    "REPO_DIR = '/content/diffusionInpaintingVectorFields'\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f'Repo already cloned at {REPO_DIR}, pulling latest...')\n",
    "    !cd {REPO_DIR} && git pull\n",
    "else:\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "\n",
    "%cd {REPO_DIR}\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab already has torch, numpy, matplotlib)\n",
    "!pip install -q tqdm pyyaml scipy gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symlink data from Google Drive into the expected location\n",
    "DRIVE_DATA = '/content/drive/MyDrive/Ocean Inpainting'\n",
    "LOCAL_DATA = f'{REPO_DIR}/data/rams_head'\n",
    "\n",
    "os.makedirs(LOCAL_DATA, exist_ok=True)\n",
    "\n",
    "# Symlink the .mat file (942 MB — don't copy, just link)\n",
    "mat_src = f'{DRIVE_DATA}/stjohn_hourly_5m_velocity_ramhead_v2.mat'\n",
    "mat_dst = f'{LOCAL_DATA}/stjohn_hourly_5m_velocity_ramhead_v2.mat'\n",
    "bounds_src = f'{DRIVE_DATA}/boundaries.yaml'\n",
    "bounds_dst = f'{LOCAL_DATA}/boundaries.yaml'\n",
    "\n",
    "for src, dst in [(mat_src, mat_dst), (bounds_src, bounds_dst)]:\n",
    "    if not os.path.exists(dst):\n",
    "        assert os.path.exists(src), f'Missing: {src}\\nUpload to Google Drive first!'\n",
    "        os.symlink(src, dst)\n",
    "        print(f'Linked {dst} → {src}')\n",
    "    else:\n",
    "        print(f'Already exists: {dst}')\n",
    "\n",
    "!ls -lh {LOCAL_DATA}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data.pickle file (train/val/test split from raw .mat)\n",
    "# This is required by DDInitializer and only needs to run once per Colab session\n",
    "import os\n",
    "PICKLE_PATH = f'{REPO_DIR}/data.pickle'\n",
    "\n",
    "if not os.path.exists(PICKLE_PATH):\n",
    "    print('Generating data.pickle from .mat file...')\n",
    "    %cd {REPO_DIR}\n",
    "    !python data_prep/spliting_data_sets.py\n",
    "    assert os.path.exists(PICKLE_PATH), 'data.pickle was not created!'\n",
    "    print(f'Created: {PICKLE_PATH}')\n",
    "else:\n",
    "    print(f'data.pickle already exists: {PICKLE_PATH}')\n",
    "\n",
    "!ls -lh {PICKLE_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835ab440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available and check memory\n",
    "import torch\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f'Memory: {mem_gb:.1f} GB')\n",
    "    # Recommend batch size based on available GPU memory\n",
    "    if mem_gb >= 40:     # A100\n",
    "        rec_bs = 128\n",
    "    elif mem_gb >= 16:   # V100 / T4\n",
    "        rec_bs = 64\n",
    "    else:\n",
    "        rec_bs = 32\n",
    "    print(f'Recommended batch_size for this GPU: {rec_bs}')\n",
    "else:\n",
    "    print('WARNING: No GPU! Go to Runtime → Change runtime type → GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2a7167",
   "metadata": {},
   "source": [
    "## 2. Validate Experiment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d1a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry-run to validate config\n",
    "!PYTHONPATH=. python experiments/run_experiment.py \\\n",
    "    --dry-run experiments/02_inpaint_algorithm/repaint_gaussian_attn/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9315ca4",
   "metadata": {},
   "source": [
    "## 3. Create Colab-Optimized Config\n",
    "\n",
    "The repo config uses `batch_size: 16` (for local MPS training).\n",
    "This cell writes a Colab-specific override with GPU-appropriate batch size\n",
    "and all training improvements (cosine LR, AdamW, EMA, augmentation).\n",
    "\n",
    "Adjust `COLAB_BATCH_SIZE` below if needed (64 for T4/V100, 128 for A100).\n",
    "\n",
    "**LR scaling:** When increasing batch size, scale LR proportionally.\n",
    "`batch_size: 64` with `lr: 0.0012` ≈ same effective LR as `batch_size: 16` with `lr: 0.0003`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Adjust this based on your GPU (see cell above) ──\n",
    "COLAB_BATCH_SIZE = 64   # 64 for T4/V100 (16 GB), 128 for A100 (40 GB)\n",
    "\n",
    "# Scale LR proportionally with batch size (linear scaling rule)\n",
    "BASE_LR = 0.0003\n",
    "BASE_BS = 16\n",
    "COLAB_LR = BASE_LR * (COLAB_BATCH_SIZE / BASE_BS)\n",
    "\n",
    "# Write a Colab-specific config that overrides batch_size for GPU\n",
    "COLAB_CFG = f'{REPO_DIR}/experiments/02_inpaint_algorithm/repaint_gaussian_attn/colab_config.yaml'\n",
    "\n",
    "with open(COLAB_CFG, 'w') as f:\n",
    "    f.write(f\"\"\"# Auto-generated Colab config — GPU-optimized with training improvements\n",
    "model_name: repaint_gaussian_attn_eps_t250\n",
    "noise_function: gaussian\n",
    "unet_type: standard_attn\n",
    "prediction_target: eps\n",
    "mask_xt: false\n",
    "batch_size: {COLAB_BATCH_SIZE}\n",
    "lr: {COLAB_LR}\n",
    "max_grad_norm: 1.0\n",
    "\n",
    "# ── Training improvements ──\n",
    "weight_decay: 0.0001       # AdamW regularization\n",
    "lr_schedule: cosine        # cosine annealing with warmup\n",
    "warmup_epochs: 10          # linear warmup from lr*0.001 → lr\n",
    "use_ema: true              # exponential moving average of weights\n",
    "ema_decay: 0.9999          # standard DDPM EMA decay\n",
    "augment: false             # disabled — site-specific ocean data shouldn't be flipped\n",
    "\"\"\")\n",
    "\n",
    "print(f'Wrote Colab config with batch_size={COLAB_BATCH_SIZE}, lr={COLAB_LR}')\n",
    "print(f'  → {COLAB_CFG}')\n",
    "!cat {COLAB_CFG}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3de3a",
   "metadata": {},
   "source": [
    "## 4. Quick Smoke Test (3 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71000dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke test — verify everything works on GPU before committing to full run\n",
    "!PYTHONPATH=. python experiments/run_experiment.py \\\n",
    "    --smoke {COLAB_CFG}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d878467",
   "metadata": {},
   "source": [
    "## 5. Full Training (1000 epochs)\n",
    "\n",
    "Uses GPU-optimized settings with all training improvements:\n",
    "- **Cosine LR** with 10-epoch warmup (avoids Adam plateau)\n",
    "- **AdamW** with weight_decay=0.0001 (regularization)\n",
    "- **EMA** decay=0.9999 (smoother inference quality)\n",
    "- **Augmentation** — velocity-aware H/V flips (4× effective dataset)\n",
    "- **Full test-set evaluation** (all 1965 samples)\n",
    "\n",
    "With Colab Pro (A100), this should take ~6-10 hours for 1000 epochs.\n",
    "With T4, expect ~15-20 hours.\n",
    "\n",
    "Checkpoints are saved automatically (best + periodic).\n",
    "\n",
    "**Tip:** Keep the browser tab active to avoid disconnection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fc07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training run\n",
    "!PYTHONPATH=. python experiments/run_experiment.py {COLAB_CFG}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b350d5",
   "metadata": {},
   "source": [
    "## 6. Save Results to Google Drive\n",
    "\n",
    "Copy checkpoints to Drive so they survive Colab shutdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bdceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "SRC = f'{REPO_DIR}/experiments/02_inpaint_algorithm/repaint_gaussian_attn/results'\n",
    "DST = '/content/drive/MyDrive/Ocean Inpainting/training_results/repaint_gaussian_attn'\n",
    "\n",
    "os.makedirs(DST, exist_ok=True)\n",
    "\n",
    "# Copy all checkpoint and log files\n",
    "copied = 0\n",
    "for f in os.listdir(SRC):\n",
    "    if f.endswith(('.pt', '.yaml', '.csv', '.png')):\n",
    "        shutil.copy2(os.path.join(SRC, f), os.path.join(DST, f))\n",
    "        print(f'  Copied: {f}')\n",
    "        copied += 1\n",
    "\n",
    "print(f'\\nCopied {copied} files to {DST}')\n",
    "!ls -lh {DST}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f20299",
   "metadata": {},
   "source": [
    "## 7. Resume Training (if interrupted)\n",
    "\n",
    "If Colab disconnected, re-run cells 1-3 (mount + clone + colab config), then run this cell.\n",
    "It resumes from the best checkpoint saved on Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcce1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# ── Adjust these to match your initial config ──\n",
    "COLAB_BATCH_SIZE = 64\n",
    "BASE_LR = 0.0003\n",
    "BASE_BS = 16\n",
    "COLAB_LR = BASE_LR * (COLAB_BATCH_SIZE / BASE_BS)\n",
    "\n",
    "# First, restore checkpoint from Drive if results folder is empty\n",
    "SRC_DRIVE = '/content/drive/MyDrive/Ocean Inpainting/training_results/repaint_gaussian_attn'\n",
    "DST_LOCAL = f'{REPO_DIR}/experiments/02_inpaint_algorithm/repaint_gaussian_attn/results'\n",
    "os.makedirs(DST_LOCAL, exist_ok=True)\n",
    "\n",
    "if os.path.exists(SRC_DRIVE):\n",
    "    for f in os.listdir(SRC_DRIVE):\n",
    "        if f.endswith('.pt'):\n",
    "            dst_path = os.path.join(DST_LOCAL, f)\n",
    "            if not os.path.exists(dst_path):\n",
    "                shutil.copy2(os.path.join(SRC_DRIVE, f), dst_path)\n",
    "                print(f'  Restored: {f}')\n",
    "\n",
    "# Find best checkpoint\n",
    "ckpt = None\n",
    "for f in os.listdir(DST_LOCAL):\n",
    "    if 'best_checkpoint' in f and f.endswith('.pt'):\n",
    "        ckpt = os.path.join(DST_LOCAL, f)\n",
    "        break\n",
    "\n",
    "if ckpt:\n",
    "    print(f'Resuming from: {ckpt}')\n",
    "    # Create a temporary resume config with all improvements\n",
    "    resume_yaml = os.path.join(DST_LOCAL, 'resume_config.yaml')\n",
    "    with open(resume_yaml, 'w') as f:\n",
    "        f.write(f\"\"\"# Auto-generated resume config with training improvements\n",
    "model_name: repaint_gaussian_attn_eps_t250\n",
    "noise_function: gaussian\n",
    "unet_type: standard_attn\n",
    "prediction_target: eps\n",
    "mask_xt: false\n",
    "batch_size: {COLAB_BATCH_SIZE}\n",
    "lr: {COLAB_LR}\n",
    "max_grad_norm: 1.0\n",
    "\n",
    "# Training improvements\n",
    "weight_decay: 0.0001\n",
    "lr_schedule: cosine\n",
    "warmup_epochs: 10\n",
    "use_ema: true\n",
    "ema_decay: 0.9999\n",
    "augment: false\n",
    "\n",
    "# Resume settings\n",
    "retrain_mode: true\n",
    "model_to_retrain: {ckpt}\n",
    "reset_best: false\n",
    "\"\"\")\n",
    "    !PYTHONPATH=. python experiments/run_experiment.py {resume_yaml}\n",
    "else:\n",
    "    print('No checkpoint found — starting fresh with Colab config')\n",
    "    # Generate config inline (no dependency on fresh-training cell)\n",
    "    COLAB_CFG = f'{REPO_DIR}/experiments/02_inpaint_algorithm/repaint_gaussian_attn/colab_config.yaml'\n",
    "    with open(COLAB_CFG, 'w') as f:\n",
    "        f.write(f\"\"\"# Auto-generated fresh Colab config with training improvements\n",
    "model_name: repaint_gaussian_attn_eps_t250\n",
    "noise_function: gaussian\n",
    "unet_type: standard_attn\n",
    "prediction_target: eps\n",
    "mask_xt: false\n",
    "batch_size: {COLAB_BATCH_SIZE}\n",
    "lr: {COLAB_LR}\n",
    "max_grad_norm: 1.0\n",
    "\n",
    "# Training improvements\n",
    "weight_decay: 0.0001\n",
    "lr_schedule: cosine\n",
    "warmup_epochs: 10\n",
    "use_ema: true\n",
    "ema_decay: 0.9999\n",
    "augment: false\n",
    "\"\"\")\n",
    "    print(f'  Wrote fresh config → {COLAB_CFG}')\n",
    "    !PYTHONPATH=. python experiments/run_experiment.py {COLAB_CFG}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e60cd8e",
   "metadata": {},
   "source": [
    "## 8. Download Best Checkpoint Locally\n",
    "\n",
    "After training completes, download the checkpoint to use on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f37d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "RESULTS = f'{REPO_DIR}/experiments/02_inpaint_algorithm/repaint_gaussian_attn/results'\n",
    "for f in os.listdir(RESULTS):\n",
    "    if 'best_checkpoint' in f and f.endswith('.pt'):\n",
    "        print(f'Downloading: {f}')\n",
    "        files.download(os.path.join(RESULTS, f))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
