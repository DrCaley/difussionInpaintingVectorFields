{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "# CombNet Pretraining on Google Colab\n\nThis notebook trains the VectorCombinationUNet (CombNet) to fix boundary divergence issues in div-free vector field inpainting.\n\n**Steps:**\n1. Upload your project code (275 KB ZIP!)\n2. Generate training data on Colab GPU (~3 min)\n3. Train model (100 epochs, ~2-3 hours on T4 GPU)\n4. Download the trained model\n\n**What to upload:**\n- Just the project ZIP file: `diffusion_tiny_v2.zip` (275 KB)\n- No ocean data needed - we generate synthetic div-free fields!"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: No GPU detected! Go to Runtime > Change runtime type > Select GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies (PyTorch is pre-installed on Colab)\n",
    "!pip install -q tqdm pyyaml scipy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_header"
   },
   "source": "## 2. Upload Project ZIP (Only 275 KB!)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_zip"
   },
   "outputs": [],
   "source": "# Upload your tiny project ZIP\nfrom google.colab import files\nimport zipfile\nimport os\n\nprint(\"Please upload diffusion_tiny_v2.zip (275 KB)\")\nuploaded = files.upload()\n\n# Extract the ZIP file\nfor filename in uploaded.keys():\n    if filename.endswith('.zip'):\n        print(f\"Extracting {filename}...\")\n        with zipfile.ZipFile(filename, 'r') as zip_ref:\n            zip_ref.extractall('.')\n        print(\"Extraction complete!\")\n        \n# Navigate to project directory\n%cd diffusionInpaintingVectorFields\n!pwd"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify_header"
   },
   "source": [
    "## 3. Verify Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_structure"
   },
   "outputs": [],
   "source": [
    "# Verify key files exist\n",
    "import os\n",
    "\n",
    "required_files = [\n",
    "    'data.yaml',\n",
    "    'scripts/pretrain_combnet.py',\n",
    "    'scripts/generate_combnet_data.py',\n",
    "    'ddpm/vector_combination/combiner_unet.py',\n",
    "    'ddpm/vector_combination/combination_loss.py',\n",
    "]\n",
    "\n",
    "print(\"Checking required files:\")\n",
    "all_good = True\n",
    "for f in required_files:\n",
    "    exists = os.path.exists(f)\n",
    "    status = \"✅\" if exists else \"❌\"\n",
    "    print(f\"{status} {f}\")\n",
    "    if not exists:\n",
    "        all_good = False\n",
    "        \n",
    "if all_good:\n",
    "    print(\"\\n✅ All required files present!\")\n",
    "else:\n",
    "    print(\"\\n❌ Some files are missing. Please check your upload.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generate_header"
   },
   "source": "## 4. Generate Training Data\n\nCreates 20,000 training samples from synthetic div-free fields. Takes ~2-3 minutes on GPU.\n\n**No ocean data needed!** We generate pure synthetic fields.\n\n**Note**: Reduced to 20k samples (from 40k) to avoid OOM on Colab free tier."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_data"
   },
   "outputs": [],
   "source": [
    "# Generate training data (completely synthetic!)\n",
    "!python scripts/generate_combnet_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_data"
   },
   "outputs": [],
   "source": [
    "# Verify dataset was created\n",
    "import torch\n",
    "dataset_path = \"results/combnet_training_data.pt\"\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    dataset = torch.load(dataset_path, weights_only=False)\n",
    "    print(\"✅ Training dataset generated successfully!\")\n",
    "    print(f\"\\nDataset contents:\")\n",
    "    for key, val in dataset.items():\n",
    "        print(f\"  {key}: {val.shape}\")\n",
    "    print(f\"\\nTotal samples: {len(dataset['known'])}\")\n",
    "    \n",
    "    # Show some statistics\n",
    "    print(f\"\\nSample statistics:\")\n",
    "    print(f\"  Field range: [{dataset['known'].min():.3f}, {dataset['known'].max():.3f}]\")\n",
    "    print(f\"  Mask coverage: {dataset['mask'].mean():.1%}\")\n",
    "else:\n",
    "    print(\"❌ Dataset generation failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_header"
   },
   "source": [
    "## 5. Run Training\n",
    "\n",
    "Train for 100 epochs (~2-3 hours on T4 GPU, ~1 hour on A100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "# Run training with tee to save logs\n",
    "!python scripts/pretrain_combnet.py --epochs 100 --batch_size 32 --lr 0.001 2>&1 | tee combnet_colab_training.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitor_header"
   },
   "source": [
    "## 6. Monitor Training Progress\n",
    "\n",
    "Run this cell while training to check progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_progress"
   },
   "outputs": [],
   "source": [
    "# Check last few lines of log\n",
    "!tail -30 combnet_colab_training.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plot_header"
   },
   "source": [
    "## 7. Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_loss"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the saved model to get training history\n",
    "checkpoint = torch.load('ddpm/Trained_Models/pretrained_combnet.pt', map_location='cpu', weights_only=False)\n",
    "\n",
    "if 'train_losses' in checkpoint:\n",
    "    losses = checkpoint['train_losses']\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses, linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('CombNet Training Loss', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_loss.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"  Total epochs: {len(losses)}\")\n",
    "    print(f\"  Initial loss: {losses[0]:.6f}\")\n",
    "    print(f\"  Final loss: {losses[-1]:.6f}\")\n",
    "    print(f\"  Best loss: {checkpoint['loss']:.6f}\")\n",
    "    print(f\"  Improvement: {(1 - losses[-1]/losses[0])*100:.1f}%\")\n",
    "else:\n",
    "    print(\"No training history found in checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_header"
   },
   "source": [
    "## 8. Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_model"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download the trained model\n",
    "print(\"Downloading trained model...\")\n",
    "files.download('ddpm/Trained_Models/pretrained_combnet.pt')\n",
    "\n",
    "# Also download the training log\n",
    "print(\"Downloading training log...\")\n",
    "files.download('combnet_colab_training.log')\n",
    "\n",
    "# Download the loss plot\n",
    "if os.path.exists('training_loss.png'):\n",
    "    print(\"Downloading loss plot...\")\n",
    "    files.download('training_loss.png')\n",
    "\n",
    "print(\"\\n✅ Downloads complete!\")\n",
    "print(\"\\nPlace the pretrained_combnet.pt file in:\")\n",
    "print(\"  ddpm/Trained_Models/pretrained_combnet.pt\")\n",
    "print(\"\\nThen set in data.yaml:\")\n",
    "print(\"  use_comb_net: auto  # or 'yes' to force using it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_header"
   },
   "source": [
    "## 9. Quick Model Test (Optional)\n",
    "\n",
    "Test that the model loads correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_model"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from ddpm.vector_combination.combiner_unet import VectorCombinationUNet\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VectorCombinationUNet(n_channels=4, n_classes=2).to(device)\n",
    "\n",
    "checkpoint = torch.load('ddpm/Trained_Models/pretrained_combnet.pt', weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")\n",
    "print(f\"\\nModel config:\")\n",
    "for key, value in checkpoint['config'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(1, 4, 64, 128).to(device)  # [batch, channels, H, W]\n",
    "with torch.no_grad():\n",
    "    output = model(test_input)\n",
    "    \n",
    "print(f\"\\nTest forward pass:\")\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Output range: [{output.min():.4f}, {output.max():.4f}]\")\n",
    "print(\"\\n✅ Model is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notes_header"
   },
   "source": "## Notes\n\n**Free Tier Limitations:**\n- Sessions timeout after ~12 hours of inactivity\n- You may be disconnected if idle\n- The model is saved every epoch, so you can resume with `--resume` flag\n\n**To Resume Training:**\n```python\n!python scripts/pretrain_combnet.py --epochs 100 --batch_size 32 --resume\n```\n\n**Batch Size Tips:**\n- T4 GPU (free tier): Use batch_size=16 or 32\n- A100 GPU (Colab Pro): Can use batch_size=64 or higher\n- If you get OOM errors, reduce batch size to 8\n\n**Key Improvements:**\n- Only 275 KB upload (vs 2.6 GB!)\n- No ocean data needed - fully synthetic training data\n- Generates 40k samples in ~3 minutes on GPU\n- Total time: ~2-3 hours on free T4 GPU"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}